"""
check for more: https://habr.com/ru/articles/669674/
llamacpp
cohere - paid

"""

from functools import partial

# pip install transformers sentencepiece
import torch
from transformers import AutoTokenizer, AutoModel, BertModel, BertTokenizerFast


def get_embedder():
    from langchain.embeddings import HuggingFaceInstructEmbeddings
    embeddings = HuggingFaceInstructEmbeddings(
            embed_instruction="Represent the each separate message for retrieval: ",
            query_instruction="represent the message"  # Represent the question for retrieving supporting texts from the messages: 
    )
    return embeddings


def get_embedder_hf():
    from langchain.embeddings import HuggingFaceEmbeddings

    model_name = "sentence-transformers/all-mpnet-base-v2"
    model_kwargs = {'device': 'cuda:0'}
    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)

    return hf


def get_embedder_rubert():
    """https://huggingface.co/cointegrated/rubert-tiny2"""
    tokenizer = AutoTokenizer.from_pretrained("cointegrated/rubert-tiny2")
    model = AutoModel.from_pretrained("cointegrated/rubert-tiny2")
    model.cuda()  # uncomment it if you have a GPU

    def embed_bert_cls(text, model, tokenizer):
        t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')
        with torch.no_grad():
                model_output = model(**{k: v.to(model.device) for k, v in t.items()})
        embeddings = model_output.last_hidden_state[:, 0, :]
        embeddings = torch.nn.functional.normalize(embeddings)
        return embeddings[0].cpu().numpy()

    return partial(embed_bert_cls, model=model, tokenizer=tokenizer)


def get_embedder_labse():
    "https://huggingface.co/setu4993/LaBSE"
    tokenizer = BertTokenizerFast.from_pretrained("setu4993/LaBSE")
    model = BertModel.from_pretrained("setu4993/LaBSE")
    model = model.eval()

    def process(sentence):
        english_inputs = tokenizer([sentence], return_tensors="pt", padding=True)

        with torch.no_grad():
            english_outputs = model(**english_inputs).pooler_output

        return english_outputs[0].numpy()  # 768
    
    return process


if __name__ == "__main__":
    # embedder = get_embedder()
    # embedder = get_embedder_rubert()
    embedder = get_embedder_labse()

#     text1 = "–≠–Ω–∫–æ–¥–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (sentence encoder) ‚Äì —ç—Ç–æ –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–µ–∫—Å—Ç–∞–º –≤–µ–∫—Ç–æ—Ä—ã –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –ø—Ä–∏—á—ë–º —Ç–∞–∫, —á—Ç–æ —É —Ç–µ–∫—Å—Ç–æ–≤, –ø–æ—Ö–æ–∂–∏—Ö –ø–æ —Å–º—ã—Å–ª—É, –∏ –≤–µ–∫—Ç–æ—Ä—ã —Ç–æ–∂–µ –ø–æ—Ö–æ–∂–∏. –û–±—ã—á–Ω–æ –¥–ª—è —ç—Ç–æ–π —Ü–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, –∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏. –û–Ω–∏ –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –∫—É—á–∏ –∑–∞–¥–∞—á, –Ω–∞–ø—Ä–∏–º–µ—Ä, few-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∏–ª–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è."
#     text2 = "–ù–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ —Ç–∞–∫–∏—Ö –ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞–Ω–∏–º–∞—é—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏ –∏–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –º–µ–¥–ª–µ–Ω–Ω–æ, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –æ–±—ã—á–Ω—ã—Ö CPU. –ú–æ–∂–Ω–æ –ª–∏ –≤—ã–±—Ä–∞—Ç—å –Ω–∞–∏–ª—É—á—à–∏–π —ç–Ω–∫–æ–¥–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å —É—á—ë—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–∞, –±—ã—Å—Ç—Ä–æ–¥–µ–π—Å—Ç–≤–∏—è, –∏ –ø–∞–º—è—Ç–∏? –Ø —Å—Ä–∞–≤–Ω–∏–ª 25 —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –Ω–∞ 8 –∑–∞–¥–∞—á–∞—Ö, –∏ —Å–æ—Å—Ç–∞–≤–∏–ª –∏—Ö —Ä–µ–π—Ç–∏–Ω–≥. –°–∞–º–æ–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –æ–∫–∞–∑–∞–ª—Å—è mUSE, —Å–∞–º–æ–π –±—ã—Å—Ç—Ä–æ–π –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö ‚Äì FastText, –∞ –ø–æ –±–∞–ª–∞–Ω—Å—É —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–±–µ–¥–∏–ª rubert-tiny2. –ö–æ–¥ –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤—ã–ª–æ–∂–µ–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ encodechka, –∞ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ ‚Äì –ø–æ–¥ –∫–∞—Ç–æ–º."
#     text3 = "–ù–µ–º–µ—Ü–∫–∏–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ –†–æ–±–µ—Ä—Ç –ö–Ω–µ—à–∫–µ –≤ —Ñ–µ–≤—Ä–∞–ª–µ —ç—Ç–æ–≥–æ –≥–æ–¥–∞ —É–∑–Ω–∞–ª, —á—Ç–æ –µ–≥–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ä–µ—Å—É—Ä—Å—É Have I Been Trained? –ö–Ω–µ—à–∫–µ –≤—ã—à–µ–ª –Ω–∞ LAION-5B, –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –±–æ–ª–µ–µ —á–µ–º 5,8 –º–ª—Ä–¥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏–π –∏–∑–≤–µ—Å—Ç–Ω–æ–π –Ω–µ–∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ LAION. –ò–º–µ–Ω–Ω–æ –µ—ë –¥–∞–Ω–Ω—ã–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ Stability AI –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Stable Diffusion. –ö–Ω–µ—à–∫–µ –æ–±–Ω–∞—Ä—É–∂–∏–ª ¬´–∫—É—á—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π¬ª –∏–∑ –µ–≥–æ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ –≤ LAION-5B –∏ –Ω–∞–ø–∏—Å–∞–ª –æ–± —ç—Ç–æ–º –≤ —Å–≤–æ—ë–º –±–ª–æ–≥–µ."
    
    text5 = "–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ —Å chatgpt"
    text4 = "–ø—Ä–æ–µ–∫—Ç —Å chatgpt"
    text1 = "–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –∑–∞–ø—Ä—ã–≥–∏–≤–∞—é –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞–≥–æ–Ω :)\n–ï—Å–ª–∏ –µ—Å—Ç—å –∂–µ–ª–∞—é—â–∏–π –ø—Ä–∏—Å–æ–µ–¥–µ–Ω–∏—Ç—å—Å—è –∫ –ø—Ä–æ–µ–∫—Ç—É - –≤—ç–ª–µ–∫–∞–º!\n\n–ü—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–æ–∑–¥–∞–Ω–∏–µ AI-–ø–æ–º–æ—â–Ω–∏–∫–∞, –ø–æ–¥–æ–±–Ω–æ–≥–æ ChatGPT, —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º - –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è –µ–≥–æ –Ω–∞ –æ–ø–∏—Å–∞–Ω–∏–∏ –±–∏–∑–Ω–µ—Å–∞, –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∞—à–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–º–æ—â–Ω–∏–∫–∞, –æ—Ç–≤–µ—á–∞—é—â–µ–≥–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É, –±–∏–∑–Ω–µ—Å—É –∏–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å—É. \n\n–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –µ–≥–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ —Å–≤–æ—é –ª–∏—á–Ω—É—é —Å–ø—Ä–∞–≤–æ—á–Ω—É—é - –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Å–≤–æ–∏ –±–∏–ª–µ—Ç—ã –Ω–∞ —Å–∞–º–æ–ª–µ—Ç—ã, –∫–æ–Ω—Ü–µ—Ä—Ç—ã, —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –ø—Ä–æ—Å—Ç–æ –∏–¥–µ–∏, –∞ –∑–∞—Ç–µ–º –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ –¥–∞—Ç–∞—Ö –ø–æ–µ–∑–¥–∫–∏ –≤ –¢—É—Ä—Ü–∏—é –∏–ª–∏ –ø–æ–ø—Ä–æ—Å–∏—Ç—å –ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –∏–¥–µ–∏ —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ —ç—Ç–æ–º –≥–æ–¥—É. \n\n–ü—Ä–∏ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–º–æ—â–Ω–∏–∫ –Ω–µ —Ç–æ–ª—å–∫–æ –¥–∞–µ—Ç –æ—Ç–≤–µ—Ç, –Ω–æ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç \"–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\" - –∏–∑–Ω–∞—á–∞—Ç—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏–ª–∏ —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º."        
    text2 = "–°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π –∏ —Å–∞–º—ã–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π - —É–∂–µ –ø–æ–¥–∫–ª—é—á–µ–Ω—ã –∫ ChatGpt ?  –í –Ω–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è–µ—à—å - \"–ø—Ä–æ—à—É —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ "
    text3 = "–ü—Ä–∏—á—ë–º, –∫–∞–∫ —è –ø–æ–Ω—è–ª —ç—Ç –∂–µ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ. –ü–∏—à–µ—à—å ChatGPT, –Ω–∞–ø–∏—à–∏ –∫–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏–ª—Å—è —á–∞—Ç –∂–ø—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ—à—å. –í—Å–µ. –î–∞—Ç–∞—Å–∞–µ–Ω—Ç–∏—Å—Ç—ã –Ω–µ –Ω—É–∂–Ω—ã –±–æ–ª—å—à–µ –Ω–∞—Ñ–∏–≥ üòÇ"

    texts = [text1, text2, text3, text4, text5]

    from itertools import permutations, combinations
    from sklearn.metrics.pairwise import cosine_similarity
    
    # embeddings = [embedder.embed_query(text.lower()) for text in texts]
    # embeddings = [embedder(text.lower()) for text in texts]
    perms = combinations(texts, 2)

    for t1, t2 in perms:
        print("====")
        print(f"Text1: {t1} \nText2: {t2}")
        # print(cosine_similarity([embedder.embed_query(t1)], [embedder.embed_query(t2)]))
        print(cosine_similarity([embedder(t1)], [embedder(t2)]))
        print("=====", end="\n\n")